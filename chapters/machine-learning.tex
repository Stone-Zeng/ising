\chapter{机器学习简介}

\section{机器学习有关思想方法}

机器学习是当今计算机科学的重要分支。它可以利用计算机的强大计算能力，从大量数据中提取特征，并构建
相应的模型。生成的模型还可以用来进行预测。

考虑最简单的回归模型。假设我们给定了一些数据点 $(\bm{x}^{(i)}, \, y^{(i)})$，其中 $\bm{x}^{(i)}$
称为特征（feature），$y^{(i)}\in\R$ 称为标签（label）。训练样本的数量为 $m$，即
$i=1,\,2,\,\ldots,\,m$。特征的数量为 $n$，但按照惯例，取 $x_0=0$，故有 $\bm{x}^{(i)}\in\R^{n+1}$。
一般而言，特征反映了样本的各种性质（如图片中的像素点），而标签则是需要需要预测的内容（如图片对应的
物体类别）。为了学习样本数据，我们需要给出一个目标函数 $h_\bm{\theta}: \R^{n+1}\to\R$，使得
$h_\bm{\theta}(\hat{\bm{x}})=\hat{y}$。这里的 $\bm{\theta}$ 称为参数（parameter），它包含了模型的
全部训练信息。

显然，如果要求学习的效果越好，就要求预测值 $\hat{y}$ 与真实值 $y$ 的差距越小。因此我们需要定义
损失函数（loss function） $L$，并使 $L$ 关于参数 $\bm{\theta}$ 取最小值。如果数据规模较小、模型比较
简单，$L$ 的最小值可以解析求得。例如线性回归模型，即
\begin{equation}
  h_\bm{\theta} (\bm{x})
  = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
  = \sum_{j=0}^n \theta_j x_j,
\end{equation}
我们取损失函数为
\begin{equation}
  L(\bm{\theta})
  = \frac{1}{2} \sum_{i=1}^m \qty\Big[y^{(i)} - h_\bm{\theta}\qty\big(\bm{x}^{(i)})]^2
  = \frac{1}{2} \norm{\bm{y} - \bm{X}\bm{\theta}}^2 \in \R.
\end{equation}
式中 $\bm{X}\in\R^{m\times(n+1)}$，且有 $X_{ij} = x^{(i)}_j$。利用最小二乘估计，可以求得最优解
\begin{equation}
  \hat{\bm{\theta}} = \qty\big(\bm{X}^\trans\bm{X})^{-1} \bm{X}^\trans \bm{y}.
\end{equation}

然而，当数据规模很大时，可以发现矩阵 $\bm{X}^\trans\bm{X}$ 的阶数也会很大，导致求其逆阵的开销无法
接受。因此可以改换思路，利用梯度下降法逼近损失函数的最小值。对于一般的多元函数，梯度给出了其在某一点
处变化率最大的方向。因而沿着梯度方向不断改变 $\bm{\theta}$ 的值，就可以逐步趋近最小值。我们可以令
\begin{equation}
  \bm{\theta} \coloneq \bm{\theta} - \alpha\nabla\bm{\theta},
\end{equation}
并不断循环，直至收敛。式中 $\alpha$ 称为学习率，用来调节每次下降的“步幅”。

在回归模型中，我们对每一组数据都给出了对应的标签。这称之为监督学习（supervised learning）。与之
相对应，另外还有无监督学习（unsupervised learning），指的是不需要为数据提供标签。无监督学习要求机器
从数据本身获取更加深刻的信息，因此相比监督学习更为困难。

除了回归问题（物理学中一般称为拟合），监督学习还包括分类等。无监督学习主要包括聚类、降维等。

一般来说，机器学习的思路可以概括如下：

\begin{enumerate}
  \item 根据数据的具体类型、特性构建相应的模型；
  \item 给出模型的损失函数；
  \item 在数据集上进行训练，找出能使损失函数取最小值的参数；
  \item 验证模型及参数的有效性，进行参数微调。
  \item 在新的数据集上测试训练好的模型。
\end{enumerate}

\section{对 Ising 模型相变的研究：线性模型}

在给定的温度下，我们可以利用 Monte Carlo 算法生成一系列 Ising 模型的自旋构型（ configuration ）。
不妨把构型作为前文所述的特征 $\bm{x}^{(i)}$，把温度作为标签 $y^{(i)}$。对大量数据样本进行训练后，
给定某一自旋构型，即可以利用机器学习的手段获知其所对应的温度。对于二维正方晶格的 Ising 模型，
其相变临界点是已知的，由此还可以知道该自旋构型处于有序态（铁磁）或是无序态（顺磁）。

\subsection{训练模型}

下面我们利用线性分类模型分析由 Monte Carlo 算法生成的 Ising 自旋构型。Ising 自旋构型是一个
$N\times N$ 的矩阵，每一个矩阵元仅可取 1 或 0。这称为布尔型数据（binary）。为了计算方便，接下来把
这一矩阵压平为一位向量，即 $\bm{x}^{(i)}\in\R^{N^2}$。对于标签 $y^{(i)}$，我们同样进行二值化，即有
\begin{equation}
  \begin{cases}
    0, & T <         \Tc; \\
    1, & T \geqslant \Tc.
  \end{cases}
\end{equation}

二值化的线性分类模型可以用下式表示：
\begin{equation}
  y = \sigma(\bm{W}^\trans \bm{x} + b).
\end{equation}
式中，$\bm{W}\in\R^{N^2}$ 称为权重（weight），$b\in\R$ 称为偏差（bias）。注意到前文的
$\bm{\theta}$ 相当于这里 $\bm{W}$ 和 $b$ 的组合，并有 $b=\theta_0$。$\sigma$ 称为激活函数
（activation function），其作用是将线性模型给出的结果映照到 $\q{0,\,1}$ 上。为了计算的方便，常把
激活函数取为逻辑函数（logistics sigmoid function），它满足
\begin{equation}
  \sigma(z) = \frac{1}{1+\ee^{-z}}.
\end{equation}
注意到 $\sigma$ 的值域为连续区间 $(0,\,1)$，我们把这一结果结果理解为 $y$ 属于 1 类别的概率
$P(y=1)$。

我们可以用网络图来描述这一模型，见图。

