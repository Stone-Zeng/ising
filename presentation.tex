\PassOptionsToPackage{log-declarations=false}{xparse}
\PassOptionsToPackage{no-math}{fontspec}
%\PassOptionsToPackage{draft}{graphicx}
\documentclass[aspectratio=169]{beamer}
\usepackage{fontspec,amsmath,unicode-math,physics,siunitx,subcaption}

\useoutertheme{metropolis}
\useinnertheme{metropolis}
\usecolortheme{metropolis}
\usefonttheme{professionalfonts}

\setbeamerfont{title}{size=\LARGE, series=\bfseries}
\setbeamerfont{frametitle}{series=\bfseries}
\setbeamerfont{caption}{size=\footnotesize}
\setbeamerfont{caption name}{series=\bfseries}
\setbeamerfont{footnote}{size=\tiny}
\setbeamertemplate{title page}{
  \begin{minipage}[b]{\textwidth}
    \mbox{}\vfill
    \usebeamertemplate*{title}
    \usebeamertemplate*{title separator}
    \usebeamertemplate*{author}
    \usebeamertemplate*{date}
    \usebeamertemplate*{institute}
    \mbox{}\hfill\inserttitlegraphic
    \vspace{-1.5cm}
  \end{minipage}
}
\setbeamertemplate{section in toc}[sections numbered]
\makeatletter
\AtBeginSection{
  \ifbeamer@inframe
    \sectionpage
  \else
    \frame[plain]{\sectionpage}
  \fi
}
\makeatother

\setsansfont{FiraGO}[BoldFont=* SemiBold]
\setmonofont{Iosevka}
\setmathfont{fira-math.otf}[Path=fonts/, BoldFont=*, math-style=ISO, bold-style=ISO, mathrm=sym]

\captionsetup[sub]{font=footnotesize}

% Constants
\def\ee{{\symup{e}}}
\def\ii{{\symup{i}}}
\def\pp{{\symup{\pi}}}
\def\kB{k_{\symup{B}}}
\def\Tc{T_{\!\symup{c}}}
% Operators
\def\incr{{\symup{\Delta}}}
\def\trans{{\symup{T}}}
\def\opT{{\symbfup{T}}}
\def\opT{{\symbfup{T}}}
\def\opO{{\symcal{O}}}
%\def\bigO{\mathscript{$\symscr{O}$}}
\def\likeL{{\symcal{L}}}
\newcommand\trR[2][]{\symbfup{R}^{\,#1}_{#2}}
%\newcommand\trA[2][]{\mathscript{$\symbfscr{A}^{\,#1}_{#2}$}}
% Set
\def\R{{\symbb{R}}}
\def\Z{{\symbb{Z}}}
\def\domD{{\symcal{D}}}
\def\domN{{\symcal{N}}}
%\def\spaceV{\mathscript{$\symscr{V}$}}
% Decorations
\def\bm#1{{\symbf{#1}}}
\def\nearest#1{\langle#1\rangle}
\def\q#1{{\{#1\}}}
% Text
\def\const{\text{const}}

\newcommand\imageinput[2][]{\includegraphics[#1]{figures/#2}}

\def\CDk{CD-$k$}
\def\AdSCFT{AdS/CFT}

\title{Machine Learning and Ising Model}
\date{\today}
\author{Xiangdong Zeng}
\institute{Department of Physics, Fudan University}
\titlegraphic{\includegraphics[height=1.2cm]{figures/fudan-emblem.pdf}}

\begin{document}

\maketitle

\iffalse
\begin{frame}{Table of contents}
\begin{itemize}
  \item \textbf{Ising model}
    \begin{itemize}
      \item Statistical physics, critical behaviors, etc
      \item Monte Carlo simulation
    \end{itemize}
  \item \textbf{Machine learning}
    \begin{itemize}
      \item Basic ideas
      \item Linear model
      \item Neural network
    \end{itemize}
  \item \textbf{Machine learning and renormalization group (RG)}
    \begin{itemize}
      \item Boltzmann machine and restricted Boltzmann machine (RBM)
      \item \CDk{} algorithm
      \item RBM for MNIST and Ising model
      \item Convolution, wavelet transform, etc
    \end{itemize}
\end{itemize}
\end{frame}

\section{Part 1. Ising model}

\begin{frame}{Ising model}
\begin{itemize}
  \item ``Binary'' spins ($+1$ or $-1$) arranged in a lattice
  \item Simplest model for (anti)-ferromagnetism and phase transition
  \item Hamiltonian:
    \[
      H(\q{\sigma_i}) = -\sum_{\nearest{ij}} J_{ij}\sigma_i\sigma_j - \mu \sum_{i} B_i\sigma_i
                      = -J \sum_{\nearest{ij}} \sigma_i\sigma_j - \mu B \sum_{i} \sigma_i
    \]
  \item Probability of configuration $\q{\sigma_i}$:
    \[
      P(\q{\sigma_i}) = \frac{\ee^{-\beta H(\q{\sigma_i})}}{Z_N} \qc
      Z_N = \sum_\q{\sigma_i} \ee^{-\beta H(\q{\sigma_i})}
    \]
\end{itemize}
\end{frame}

\begin{frame}{Exact solutions}
\begin{itemize}
  \item One dimension: no phase transition
  \item Two dimension
    \begin{itemize}
      \item Heat capacity (logarithmic divergence):
        \[ \frac{C}{N} \simeq \num{-0.4945} \ln\abs{1-\frac{T}{\Tc}} + \const \]
      \item Spontaneous magnetization:
        \[
          \frac{\bar{M}}{N\mu} =
          \begin{cases}
            \qty(1 - \sinh^{-4} 2\beta J)^{1/8}, & T<\Tc \\
            0, & T>\Tc
          \end{cases}
        \]
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Critical behaviors and RG}
\begin{itemize}
  \item Universality: $\xi\to\infty$ at critical point
  \item Partition function is scale-invariant
    \begin{itemize}
      \item Partition function:
        \[ Z_N = \sum_\q{\sigma_i} \ee^{-\beta H(\q{\sigma_i},\,\bm{K})} \]
      \item Hamiltonian:
        \[
            H(\q{\sigma_i},\,\bm{K})
          = - \sum_i K^{(1)}_i \sigma_i
            - \sum_{\nearest{ij}}  K^{(2)}_{ij}  \sigma_i\sigma_j
            - \sum_{\nearest{ijk}} K^{(3)}_{ijk} \sigma_i\sigma_j\sigma_k - \cdots
        \]
      \item Scale transformation:
        \[ N' = L^{-d} N \qc \xi' = L^{-1} \xi \]
      \item Summation over Kadanoff's block $\q{\sigma_i'}$:
        \[
          Z_{N'}
          = \sum_\q{\sigma'_i} \ee^{-\beta H^{\text{RG}}(\q{\sigma_i'},\,\bm{K}')}
          = \sum_\q{\sigma'_i} \sum_\q{\sigma_i}
            \ee^{-\beta \qty\big[H(\q{\sigma_i},\,\bm{K})
                 -\opT_\lambda(\q{\sigma_i},\,\q{\sigma_i'})]} \qc
          \bm{K}' = \trR{L}(\bm{K})
        \]
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Monte Carlo simulation}
\small
\begin{itemize}
  \item Basic idea: average over samples instead of all configurations
  \item Distribution evolution:
    \begin{align*}
      P\qty\big(\q{\sigma_i},\,t+1) = P\qty\big(\q{\sigma_i},\,t)
      &+ \sum_{\q{\sigma'_i}} P\qty\big(\q{\sigma'_i},\,t) W\qty\big(\q{\sigma'_i}\to\q{\sigma_i})
         \notag \\
      &- \sum_{\q{\sigma'_i}} P\qty\big(\q{\sigma_i},\,t)  W\qty\big(\q{\sigma_i}\to\q{\sigma'_i})
    \end{align*}
  \item Detailed balance condition:
    \[
        P_{\text{eq}}(\q{\sigma_i})  W\qty\big(\q{\sigma_i}\to\q{\sigma'_i})
      = P_{\text{eq}}(\q{\sigma'_i}) W\qty\big(\q{\sigma'_i}\to\q{\sigma_i})
    \]
  \item Metropolis transition rates:
    \[
      W\qty\big(\q{\sigma_i}\to\q{\sigma'_i}) =
      \begin{cases}
        1, & \incr{E} \leqslant 0 \\
        \ee^{-\beta \incr{E}}, & \incr{E} > 0
      \end{cases}
    \]
\end{itemize}
\end{frame}

\begin{frame}{Algorithm}
\begin{enumerate}
  \item Initialize all the spins: set spins to be 0 or 1 \emph{randomly}.
  \item Generate a trial state $\q{\sigma'_i}$ that is near the current state $\q{\sigma_i}$
    \begin{itemize}
      \item Usually we can flip a single spin.
    \end{itemize}
  \item Calculate $\incr{E}$ and accept probability $P$. If random number $\xi<P$, then flip the
    corresponding spin.
  \item Perform step 2 and 3 for all the lattice (one Monte Carlo step, MCS).
    \begin{itemize}
      \item To maintain detailed balance, we should choose the spin \emph{randomly}.
      \item Flipping one-by-one is also acceptable for efficiency.
    \end{itemize}
  \item Repeat step 2--4 for some time, then we can measure the thermodynamic observables and
    calculate their mean values and standard errors.
\end{enumerate}
\end{frame}

\begin{frame}{Simulation results (1)}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \imageinput[width=3cm]{ising-lattice-1-0.pdf}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \imageinput[width=3cm]{ising-lattice-2-3.pdf}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \imageinput[width=3cm]{ising-lattice-3-6.pdf}
    \caption{}
  \end{subfigure}
  \caption{$64 \times 64$ Ising lattice at (a) $\Tc/J=1.0$; (b) $\Tc/J=2.3$; (c) $\Tc/J=3.6$.}
\end{figure}
\end{frame}

\begin{frame}{Simulation results (2)}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \imageinput[height=3.4cm]{ising-energy}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \imageinput[height=3.4cm]{ising-magnet}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \imageinput[height=3.4cm]{ising-cv}
  \end{subfigure}
  \caption{(a) Energy; (b) spontaneous magnetization; (c) heat capacity.
    Sampling density is increased near $\Tc$. Error bar means one $\sigma$.}
\end{figure}
\end{frame}

\section{Part 2. Machine learning}

\begin{frame}{Basic ideas}
\begin{itemize}
  \item Extract features from data, build models, make predictions or decisions with the help of
    computers
  \item Give machine the ability to ``learn''
  \item Categories:
    \begin{itemize}
      \item Supervised learning: regression, classification, etc
      \item Unsupervised learning: clustering, dimensionality reduction, etc
    \end{itemize}
  \item Main steps:
    \begin{enumerate}
      \item Build a reasonable model based on data's feature;
      \item Give the \alert{loss function};
      \item Train on the dataset (training set), find the parameters to minimize the loss function;
      \item Validate the model and parameters, fine tuning (validation set);
      \item Test the trained model on \emph{new} dataset (test set).
    \end{enumerate}
\end{itemize}  
\end{frame}

\begin{frame}{Linear regression}
\begin{itemize}
  \item Hypothesis:
    \[ h_\bm{\theta} (\bm{x}) = \sum_{j=0}^n \theta_j x_j = \bm{\theta}^\trans \bm{x} \]
  \item Loss function:
    \[
      L(\bm{\theta})
      = \frac{1}{2} \sum_{i=1}^m \qty\Big[y^{(i)} - h_\bm{\theta}\qty\big(\bm{x}^{(i)})]^2
    \]
  \item Exact solution:
    \[ \hat{\bm{\theta}} = \qty\big(\bm{X}^\trans\bm{X})^{-1} \bm{X}^\trans \bm{y} \]
  \item Not realistic with large dataset --- \alert{gradient descent}
    \[ \bm{\theta} \coloneq \bm{\theta} - \alpha \, \nabla\bm{\theta} \]
\end{itemize}
\end{frame}

\begin{frame}{Linear classifier (logistic regression)}
\begin{columns}[t]
  \begin{column}{0.52\textwidth}
    \begin{itemize}
      \item Hypothesis:
        \[ h_\bm{\theta} (\bm{x}) = \sigma(\bm{\theta}^\trans \bm{x}) \]
      \item Activation function:
        \[ \sigma(z) = \frac{1}{1+\ee^{-z}} \]
      \item Loss function:
        \[
          L(\bm{\theta})
          = \sum_{i=1}^m \log\qty\Big{1+\exp[-y^{(i)} h_\bm{\theta}\qty\big(\bm{x}^{(i)})]}
        \]
    \end{itemize}
  \end{column}
  \begin{column}{0.44\textwidth}
    \begin{figure}
      \centering
      \imageinput[width=0.9\textwidth]{linear-layer}
      \caption{Network structure of the linear classifier}
    \end{figure}
  \end{column}
\end{columns}
\end{frame}

\begin{frame}{Neural network}
\begin{columns}[t]
  \begin{column}{0.44\textwidth}
    \begin{figure}
      \centering
      \imageinput[height=3.8cm]{multipolar-neuron.png}
      \caption{A multipolar neuron \footnotemark}
    \end{figure}
  \end{column}
  \begin{column}{0.54\textwidth}
    \begin{figure}
      \centering
      \imageinput[height=3.8cm]{neural-net}
      \caption{A two-layer neural network (perceptron)}
    \end{figure}
  \end{column}
\end{columns}
\footnotetext{Source: \url{https://en.wikipedia.org/wiki/File:Blausen_0657_MultipolarNeuron.png}}
\end{frame}

\begin{frame}{Neural network --- details}
\begin{columns}[t]
  \makebox[0.52\textwidth][l]{%
    \begin{column}{0.6\textwidth}
      \begin{itemize}
        \item Loss function (mean squared error):
          \[ L = \frac{1}{2} \sum_{i=1}^m \qty\big[y^{(i)} - \hat{y}^{(i)}]^2 \]
        \item Loss function (cross-entropy):
          \[
            L = - \frac{1}{m} \sum_{i=1}^m
                  \qty\Big[y^{(i)}\ln\hat{y}^{(i)}
                           - \qty\big(1-y^{(i)})\ln\qty\big(1-\hat{y}^{(i)})]^2
          \]
        \item Training algorithm: backpropagation
      \end{itemize}
    \end{column}}
  \begin{column}{0.48\textwidth}
    \begin{itemize}
      \item Regularization
        \begin{itemize}
          \item Prevent overfitting
          \item A kind of ``penalty'': force the parameters to be sparse
          \item Weight decay ($L^2$ reg\-u\-lar\-iza\-tion):
            \[ \frac{\lambda}{2m} \sum_k \norm{\bm{\theta}_k}^2 \]
        \end{itemize}
    \end{itemize}
  \end{column}
\end{columns}
\end{frame}

\begin{frame}{Training results (linear model)}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \imageinput[width=4.5cm]{ising-learning-linear.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \imageinput[width=4.5cm]{ising-learning-linear-scaled.pdf}
  \end{subfigure}
  \caption{Predict results on Ising lattice. (a) Original; (b) after scale transformation.}
\end{figure}
\end{frame}

\begin{frame}{Training results (neural network)}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \imageinput[width=4.5cm]{ising-learning-net.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \imageinput[width=4.5cm]{ising-learning-net-scaled.pdf}
  \end{subfigure}
  \caption{Predict results on Ising lattice. (a) Original; (b) after scale transformation.}
\end{figure}
\end{frame}
\fi

\section{Part 3. Machine learning and RG}

\begin{frame}{Energy-based model}
\begin{columns}[t]
  \makebox[0.42\textwidth][l]{%
    \begin{column}{0.48\textwidth}
      \begin{itemize}
        \item Boltzmann machine
          \begin{itemize}
            \item Energy function:
              \begin{align*}
                E(\bm{s}) &= E(\q{s_i}) \\
                          &= - \sum_{i<j} W_{ij} s_i s_j - \sum_i \theta_i s_i
              \end{align*}
            \item Structure:
              \begin{figure}
                \centering
                \imageinput[scale=0.6]{boltzmann-machine.pdf}
              \end{figure}
          \end{itemize}
      \end{itemize}
    \end{column}}
  \begin{column}{0.56\textwidth}
    \begin{itemize}
      \item Restricted Boltzmann machine (RBM)
        \begin{itemize}
          \item Energy function:
            \begin{align*}
              E(\bm{v},\,\bm{h})
              &= - \bm{v}^\trans \bm{W} \bm{h} - \bm{b}^\trans \bm{v} - \bm{c}^\trans \bm{h} \\
              &= - \sum_{i,\,j} W_{ij} v_i h_j - \sum_i b_i v_i - \sum_j c_j h_j
            \end{align*}
          \item Structure:
            \begin{figure}
              \centering
              \imageinput[scale=0.6]{rbm.pdf}
            \end{figure}
        \end{itemize}
    \end{itemize}
  \end{column}
\end{columns}
\end{frame}

\end{document}
